{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e99f16",
   "metadata": {},
   "source": [
    "# Notebook 03: Final Model Inference & 2024 Forecast\n",
    "## Objective\n",
    "This notebook's purpose is to generate the final, production-ready demand forecast for 2024.\n",
    "\n",
    "## Methodology\n",
    "Based on the critical findings from Notebook 02, we concluded:\n",
    "\n",
    "1. The complex \"Hybrid Model\" (GBR + Risk) was financially unviable.\n",
    "\n",
    "2. The \"GBR Base Model\" was the clear winner, delivering +$610M in net benefit over the client's current model.\n",
    "\n",
    "Therefore, our inference pipeline is now simple and robust:\n",
    "\n",
    "1. Load the full historical dataset (2012-2023).\n",
    "\n",
    "2. Load the validated hyperparameters from our gbr_model.joblib artifact (from Notebook 01).\n",
    "\n",
    "3. Re-train the GBR model on the entire dataset to maximize its accuracy.\n",
    "\n",
    "4. Generate the 2024 forecast.\n",
    "\n",
    "5. Save this forecast as the final, processed output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a91f5",
   "metadata": {},
   "source": [
    "## 0. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e070443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acacf53b",
   "metadata": {},
   "source": [
    "## 1. Define Feature Engineering Function\n",
    "This function must be identical to the one used in 01-model_core.ipynb to ensure consistency between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b5a7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    \"\"\"\n",
    "    Creates time-series features from the input dataframe.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with 'fecha', 'prod_id', 'ventas', 'precio_promedio'.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with new features.\n",
    "    \"\"\"\n",
    "    df_feat = data.sort_values(['prod_id', 'fecha']).copy()\n",
    "    \n",
    "    # --- Calendar Features ---\n",
    "    df_feat['year'] = df_feat['fecha'].dt.year\n",
    "    df_feat['month'] = df_feat['fecha'].dt.month\n",
    "    \n",
    "    # --- Lag Features (Annual) ---\n",
    "    df_feat['lag_12'] = df_feat.groupby('prod_id')['ventas'].shift(12)\n",
    "    df_feat['lag_13'] = df_feat.groupby('prod_id')['ventas'].shift(13)\n",
    "    df_feat['lag_24'] = df_feat.groupby('prod_id')['ventas'].shift(24)\n",
    "    \n",
    "    # --- Rolling Window Features ---\n",
    "    df_feat['rolling_mean_3_lag12'] = df_feat.groupby('prod_id')['lag_12'].transform(lambda x: x.rolling(3).mean())\n",
    "    \n",
    "    # --- Price Features ---\n",
    "    df_feat['precio_lag_12'] = df_feat.groupby('prod_id')['precio_promedio'].shift(12)\n",
    "    \n",
    "    return df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe33a0c",
   "metadata": {},
   "source": [
    "## 2. Load Data and Model Parameters\n",
    "We load the full history to train on, and the parameters from the model we validated in Notebook 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d081035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full historical data loaded: 80748 rows.\n",
      "Model parameters loaded from: ../models/gbr_model.joblib\n",
      "Using parameters: LR=0.05, N_Estimators=500, Max_Depth=7\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load ALL historical data ---\n",
    "RAW_DATA_PATH = '../data/raw/demanding_forecast.csv'\n",
    "df_history = pd.read_csv(RAW_DATA_PATH)\n",
    "df_history['fecha'] = pd.to_datetime(df_history['fecha'])\n",
    "print(f\"Full historical data loaded: {len(df_history)} rows.\")\n",
    "\n",
    "# --- 2. Load validated model parameters ---\n",
    "MODEL_PATH = '../models/gbr_model.joblib'\n",
    "gbr_validated = joblib.load(MODEL_PATH)\n",
    "model_params = gbr_validated.get_params()\n",
    "\n",
    "print(f\"Model parameters loaded from: {MODEL_PATH}\")\n",
    "print(f\"Using parameters: LR={model_params['learning_rate']}, N_Estimators={model_params['n_estimators']}, Max_Depth={model_params['max_depth']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6197997d",
   "metadata": {},
   "source": [
    "## 3. Re-train the Final Model\n",
    "This is the key production step. We re-train a new GBR model using our validated parameters on 100% of the historical data (2012-2023). This ensures the 2024 forecast is as accurate as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aba1f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-training model on 57420 rows (full 2012-2023 history).\n",
      "Final model re-training complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Create features on the full historical dataset ---\n",
    "df_model = create_features(df_history)\n",
    "df_model = df_model.dropna()\n",
    "\n",
    "# --- 2. Define the FINAL training set (all data <= 2023) ---\n",
    "TARGET_VARIABLE = 'ventas'\n",
    "FEATURES = [\n",
    "    'month', 'year', 'prod_id', \n",
    "    'lag_12', 'lag_13', 'lag_24', \n",
    "    'rolling_mean_3_lag12', 'precio_lag_12'\n",
    "]\n",
    "\n",
    "train_full = df_model[df_model['year'] <= 2023]\n",
    "print(f\"Re-training model on {len(train_full)} rows (full 2012-2023 history).\")\n",
    "\n",
    "# --- 3. Initialize and train the final model ---\n",
    "gbr_final = GradientBoostingRegressor(**model_params)\n",
    "\n",
    "# IMPORTANT: Disable early stopping for the final fit\n",
    "# We set n_iter_no_change=None so it trains on the full n_estimators\n",
    "gbr_final.set_params(n_iter_no_change=None)\n",
    "\n",
    "# Fit the final model\n",
    "gbr_final.fit(train_full[FEATURES], train_full[TARGET_VARIABLE])\n",
    "\n",
    "print(\"Final model re-training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ccb5b3",
   "metadata": {},
   "source": [
    "## 4. Create 2024 Forecast Scaffold\n",
    "We now build the \"scaffold\" (the empty rows for 2024), join it to the history to create the necessary lag features, and then predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c768fb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024 forecast generated.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Create future dataframe for 2024 ---\n",
    "future_dates = pd.date_range(start='2024-01-01', end='2024-12-01', freq='MS')\n",
    "prod_ids = df_history['prod_id'].unique()\n",
    "\n",
    "df_future_rows = []\n",
    "for pid in prod_ids:\n",
    "    for date in future_dates:\n",
    "        df_future_rows.append({\n",
    "            'fecha': date, 'prod_id': pid,\n",
    "            'ventas': np.nan, 'precio_promedio': np.nan\n",
    "        })\n",
    "\n",
    "df_future = pd.DataFrame(df_future_rows)\n",
    "\n",
    "# --- 2. Concatenate history + future to create features ---\n",
    "df_full_history = pd.concat([df_history, df_future], axis=0) \n",
    "df_full_features = create_features(df_full_history) \n",
    "\n",
    "# --- 3. Filter to get just the 2024 rows (now with features) ---\n",
    "df_2024 = df_full_features[df_full_features['fecha'].dt.year == 2024].copy()\n",
    "\n",
    "# --- 4. Predict ---\n",
    "# Fill NaNs in features (e.g., for products with <2 years history) just in case\n",
    "df_2024[FEATURES] = df_2024[FEATURES].fillna(0)\n",
    "df_2024['prediccion_ventas'] = gbr_final.predict(df_2024[FEATURES])\n",
    "\n",
    "print(\"2024 forecast generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a86a5d",
   "metadata": {},
   "source": [
    "## 5. Save Processed Output\n",
    "This is the final forecast file, which will be used in the reporting notebook. In a production environment, this file would be loaded into the client's ERP or planning system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8390468d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024 forecast saved to: ../data/processed/demand_forecasts_2024.csv\n",
      "       fecha  prod_id  prediccion_ventas\n",
      "0 2024-01-01        0        2059.984012\n",
      "1 2024-02-01        0        1472.463920\n",
      "2 2024-03-01        0        1106.306216\n",
      "3 2024-04-01        0         880.477605\n",
      "4 2024-05-01        0         827.766047\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Save Processed Data ---\n",
    "PROCESSED_DATA_PATH = '../data/processed/demand_forecasts_2024.csv'\n",
    "output_cols = ['fecha', 'prod_id', 'prediccion_ventas']\n",
    "\n",
    "df_2024[output_cols].to_csv(PROCESSED_DATA_PATH, index=False)\n",
    "\n",
    "print(f\"2024 forecast saved to: {PROCESSED_DATA_PATH}\")\n",
    "print(df_2024[output_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71434b0e",
   "metadata": {},
   "source": [
    "## 6. Project Conclusion (Next Steps)\n",
    "\n",
    "This notebook completes the core inference pipeline.\n",
    "\n",
    "**Business Action Plan**:\n",
    "\n",
    "- The file predicciones_demanda_2024.csv represents the final recommended inventory order quantity (after rounding).\n",
    "\n",
    "- We do NOT add a statistical safety stock. Our research in Notebook 02 proved this was financially unviable due to extreme data outliers.\n",
    "\n",
    "- The $610M in value comes from replacing the client's model_actual with this more precise GBR forecast.\n",
    "\n",
    "Next Notebook: 04-generate_final_report.ipynb, where we will load this forecast and the historical data to create the final visualizations for the business."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demand_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
