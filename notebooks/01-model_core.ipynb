{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b028e2",
   "metadata": {},
   "source": [
    "# Notebook 01: Entrenamiento y Validación del Modelo Core (GBR)\n",
    "## Objetivo:\n",
    "\n",
    "- Entrenar un modelo base (Gradient Boosting) para predecir la media de la demanda.\n",
    "\n",
    "- Validar el rendimiento de este modelo contra los datos de 2023 y compararlo con el modelo_actual.\n",
    "\n",
    "- Guardar el modelo entrenado y los datos de validación para los siguientes notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eec26e",
   "metadata": {},
   "source": [
    "## 0. Configuración e Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233adeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib # Para guardar el modelo\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f94eb0",
   "metadata": {},
   "source": [
    "## 1. Carga y Exploración de Datos (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset (ajusta la ruta según tu estructura)\n",
    "path_datos = '../data/raw/demanding_forecast.csv'\n",
    "\n",
    "df = pd.read_csv(path_datos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna 'fecha' a datetime\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec2a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen estadístico\n",
    "num_productos = df['prod_id'].nunique()\n",
    "fecha_min = df['fecha'].min()\n",
    "fecha_max = df['fecha'].max()\n",
    "\n",
    "print(f\"Número de productos únicos: {num_productos}\")\n",
    "print(f\"Rango de fechas: {fecha_min.date()} a {fecha_max.date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27dc65",
   "metadata": {},
   "source": [
    "## Análisis de la Variable Objetivo (ventas)\n",
    "Buscamos evidencia de \"colas pesadas\" (eventos extremos) que justifiquen nuestro modelo de riesgo (GPD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.boxplot(x=df['ventas'], fliersize=2)\n",
    "plt.title('Distribución de Ventas (Outliers y Colas Pesadas)')\n",
    "plt.xlabel('Ventas')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.histplot(df['ventas'], bins=100, kde=True)\n",
    "plt.title('Histograma de Ventas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8dd33",
   "metadata": {},
   "source": [
    "Observación de EDA: El boxplot y el histograma muestran una fuerte asimetría positiva y una gran cantidad de valores atípicos (cola derecha pesada). Esto confirma que un modelo que solo predice la media (como un GBR) será insuficiente para gestionar el riesgo de picos de demanda. La idea del modelo híbrido con GPD es correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87898b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar 3 series de tiempo de ejemplo\n",
    "productos_ejemplo = df['prod_id'].unique()[:3]\n",
    "\n",
    "for pid in productos_ejemplo:\n",
    "    subset = df[df['prod_id'] == pid]\n",
    "    plt.plot(subset['fecha'], subset['ventas'], label=f'Producto {pid}')\n",
    "\n",
    "plt.title('Series de Tiempo (Ejemplo 3 Productos)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a27c08",
   "metadata": {},
   "source": [
    "## 2. Ingeniería de Características (Feature Engineering)\n",
    "Crearemos características que el modelo pueda usar. Dado que predecimos 2024 (un año completo), no podemos usar lags recientes (como lag_1 o lag_6) porque requerirían predicciones recursivas. Usaremos lags anuales, que son muy robustos para datos con estacionalidad anual.\n",
    "\n",
    "Crearemos lags anuales (retrasos de 12 y 24 meses) para capturar la estacionalidad sin depender de predicciones recursivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e109c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    \"\"\"Crea características de series de tiempo en el dataframe.\"\"\"\n",
    "    data = data.sort_values(['prod_id', 'fecha']).copy()\n",
    "    \n",
    "    # Features de Calendario\n",
    "    data['year'] = data['fecha'].dt.year\n",
    "    data['month'] = data['fecha'].dt.month\n",
    "    \n",
    "    # Lags Anuales (robustez para predicción a 12 meses)\n",
    "    data['lag_12'] = data.groupby('prod_id')['ventas'].shift(12)\n",
    "    data['lag_13'] = data.groupby('prod_id')['ventas'].shift(13)\n",
    "    data['lag_24'] = data.groupby('prod_id')['ventas'].shift(24)\n",
    "    \n",
    "    # Media móvil sobre el lag para suavizar\n",
    "    data['rolling_mean_3_lag12'] = data.groupby('prod_id')['lag_12'].transform(lambda x: x.rolling(3).mean())\n",
    "    \n",
    "    # Lag del precio (usamos el precio de hace un año como proxy)\n",
    "    data['precio_lag_12'] = data.groupby('prod_id')['precio_promedio'].shift(12)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Aplicar la creación de features\n",
    "df_model = create_features(df)\n",
    "\n",
    "# Eliminar filas con NaNs generados por los lags\n",
    "df_model = df_model.dropna()\n",
    "\n",
    "print(\"DataFrame con nuevas características:\")\n",
    "print(df_model.shape)\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c7270",
   "metadata": {},
   "source": [
    "## 3. Construcción del Modelo (Gradient Boosting)\n",
    "Definimos el conjunto de entrenamiento y validación. Usaremos todos los datos hasta 2022 para entrenar, y el año 2023 completo para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir features y target\n",
    "features = ['month', 'year', 'prod_id', \n",
    "            'lag_12', 'lag_13', 'lag_24', \n",
    "            'rolling_mean_3_lag12', 'precio_lag_12']\n",
    "target = 'ventas'\n",
    "\n",
    "# Dividir Train (<= 2022) y Validation (== 2023)\n",
    "train = df_model[df_model['year'] < 2023]\n",
    "val = df_model[df_model['year'] == 2023]\n",
    "\n",
    "print(f\"Filas de entrenamiento: {len(train)}\")\n",
    "print(f\"Filas de validación: {len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar y entrenar el modelo\n",
    "print(\"Entrenando el modelo GBR...\")\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=500,       # Número de árboles\n",
    "    learning_rate=0.05,     # Tasa de aprendizaje\n",
    "    max_depth=7,            # Profundidad de los árboles\n",
    "    random_state=42,\n",
    "    n_iter_no_change=20,    # Parada temprana (Early Stopping)\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "gbr.fit(train[features], train[target])\n",
    "\n",
    "print(\"¡Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efeb62f",
   "metadata": {},
   "source": [
    "## 4. Guardar el Modelo Entrenado\n",
    "Guardamos el objeto del modelo GBR en la carpeta models/ para usarlo en el notebook de inferencia (03-model_inference.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104215a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "path_modelo = '../models/gbr_model.joblib'\n",
    "joblib.dump(gbr, path_modelo)\n",
    "\n",
    "print(f\"Modelo guardado en: {path_modelo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b03d20",
   "metadata": {},
   "source": [
    "## 5. Validación y Comparativa (vs. modelo_actual)\n",
    "Usamos el set de validación (2023) para medir el rendimiento de nuestro modelo (WAPE y RMSE) y compararlo con el modelo_actual del cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Predecir con nuestro nuevo modelo (GBR)\n",
    "val_preds = gbr.predict(val[features])\n",
    "\n",
    "# 2. 'val' ya tiene las 'ventas' reales Y el 'modelo_actual'.\n",
    "#    Solo necesitamos añadirle nuestra nueva predicción.\n",
    "val_comparativo = val.copy()\n",
    "val_comparativo['pred_gbr'] = val_preds\n",
    "\n",
    "# 3. Calcular Métricas\n",
    "# 'val_comparativo' ahora tiene:\n",
    "# - 'ventas' (el valor real)\n",
    "# - 'modelo_actual' (la predicción original, que ya estaba en 'val')\n",
    "# - 'pred_gbr' (nuestra nueva predicción)\n",
    "\n",
    "# RMSE (Root Mean Squared Error)\n",
    "rmse_gbr = np.sqrt(mean_squared_error(val_comparativo['ventas'], val_comparativo['pred_gbr']))\n",
    "rmse_actual = np.sqrt(mean_squared_error(val_comparativo['ventas'], val_comparativo['modelo_actual']))\n",
    "\n",
    "# WAPE (Weighted Absolute Percentage Error) - Métrica clave de negocio\n",
    "wape_gbr = np.sum(np.abs(val_comparativo['ventas'] - val_comparativo['pred_gbr'])) / np.sum(np.abs(val_comparativo['ventas']))\n",
    "wape_actual = np.sum(np.abs(val_comparativo['ventas'] - val_comparativo['modelo_actual'])) / np.sum(np.abs(val_comparativo['ventas']))\n",
    "\n",
    "# Resultados\n",
    "print(\"--- Comparativa de Modelos (Validación 2023) ---\")\n",
    "print(f\"RMSE Modelo Actual: {rmse_actual:.2f}\")\n",
    "print(f\"RMSE Nuevo Modelo (GBR): {rmse_gbr:.2f}\")\n",
    "print(\"\\n\")\n",
    "print(f\"WAPE Modelo Actual: {wape_actual:.2%}\")\n",
    "print(f\"WAPE Nuevo Modelo (GBR): {wape_gbr:.2%}\")\n",
    "print(\"\\n\")\n",
    "mejora_wape = (wape_actual - wape_gbr) / wape_actual\n",
    "print(f\"==> Mejora en WAPE (Precisión de Negocio): {mejora_wape:.2%} ==\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b2a4f",
   "metadata": {},
   "source": [
    "## 6. Guardar Salida de Validación (para Notebook GPD)\n",
    "Guardamos el val_comparativo (que contiene los errores/residuos) en data/interim/. Este archivo es el input para nuestro notebook 02-model_risk_gpd.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bbd0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GUARDAR SALIDA PARA MODELO DE RIESGO ---\n",
    "\n",
    "# El DataFrame 'val_comparativo' contiene todo lo que necesitamos:\n",
    "# 'ventas' (Reales), 'pred_gbr' (Predicción Core)\n",
    "\n",
    "# Definir la ruta de salida (siguiendo la estructura Cookiecutter)\n",
    "path_salida_interim = '../data/interim/model_core_validation.csv'\n",
    "\n",
    "# Guardar el DataFrame\n",
    "val_comparativo.to_csv(path_salida_interim, index=False)\n",
    "\n",
    "print(f\"Resultados de validación (con predicciones) guardados en: {path_salida_interim}\")\n",
    "print(val_comparativo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69760f",
   "metadata": {},
   "source": [
    "## 5. Generación de Predicciones 2024\n",
    "Ahora que el modelo está entrenado y validado, creamos el \"scaffolding\" (andamio) para 2024, aplicamos la ingeniería de características y predecimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Crear el dataframe futuro para 2024\n",
    "future_dates = pd.date_range(start='2024-01-01', end='2024-12-01', freq='MS')\n",
    "prod_ids = df['prod_id'].unique()\n",
    "\n",
    "df_future_rows = []\n",
    "for pid in prod_ids:\n",
    "    for date in future_dates:\n",
    "        df_future_rows.append({\n",
    "            'fecha': date,\n",
    "            'prod_id': pid,\n",
    "            'ventas': np.nan,          # A predecir\n",
    "            'precio_promedio': np.nan # No lo usamos directamente\n",
    "        })\n",
    "\n",
    "df_future = pd.DataFrame(df_future_rows)\n",
    "\n",
    "# 2. Concatenar historial + futuro para crear features\n",
    "df_full = pd.concat([df, df_future], axis=0)\n",
    "\n",
    "# 3. Aplicar ingeniería de características\n",
    "df_full_features = create_features(df_full)\n",
    "\n",
    "# 4. Filtrar solo el 2024 (que ahora tiene features como lag_12)\n",
    "df_2024 = df_full_features[df_full_features['fecha'].dt.year == 2024].copy()\n",
    "\n",
    "# 5. Predecir\n",
    "# (Puede haber nulos en features si un producto es nuevo, por seguridad llenamos con 0)\n",
    "df_2024[features] = df_2024[features].fillna(0)\n",
    "\n",
    "df_2024['prediccion_ventas'] = gbr.predict(df_2024[features])\n",
    "\n",
    "# 6. Guardar el CSV\n",
    "# En una estructura Cookiecutter, esto iría a 'data/processed/' o 'models/'\n",
    "path_salida = 'predicciones_demanda_2024.csv' # Ajusta la ruta\n",
    "\n",
    "output_cols = ['fecha', 'prod_id', 'prediccion_ventas']\n",
    "df_2024[output_cols].to_csv(path_salida, index=False)\n",
    "\n",
    "print(f\"Predicciones 2024 generadas y guardadas en '{path_salida}'\")\n",
    "df_2024[output_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17334c3f",
   "metadata": {},
   "source": [
    "## 6. Próximos Pasos: Hacia el Modelo Híbrido (GBR + GPD)\n",
    "Este notebook ha construido con éxito un Modelo Core que predice la media de la demanda con mayor precisión (menor WAPE) que el modelo actual.\n",
    "\n",
    "Sin embargo, como vimos en el EDA, el negocio no está en la media, está en los extremos. Los costos de quiebre de stock (subestimar) y desperdicio (sobreestimar) son asimétricos y ocurren en las colas de la distribución.\n",
    "\n",
    "El siguiente paso lógico es usar las predicciones de este modelo (pred_gbr) y los valores reales para calcular los residuos (errores).\n",
    "\n",
    "Residuo = Venta_Real - Predicción_GBR\n",
    "\n",
    "Sobre la distribución de estos residuos (específicamente, la cola positiva) aplicaremos un modelo de Teoría de Valores Extremos (GPD - Generalized Pareto Distribution). Esto nos permitirá dejar de preguntar \"¿Cuánto vamos a vender?\" y empezar a preguntar:\n",
    "\n",
    "\"Dado que predecimos vender 1000 unidades (GBR), ¿cuál es el stock de seguridad necesario para tener un 98% de probabilidad de cubrir la demanda real? (GPD)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demand_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
